#!/usr/bin/env python3
"""
Bio-Agent æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬

å°†é€’é€ç³»ç»Ÿå’Œå¾®ç”Ÿç‰© CSV æ•°æ®å¯¼å…¥ MongoDB

ä½¿ç”¨æ–¹æ³•:
    cd backend
    python scripts/init_database.py [--dry-run] [--skip-md-check]

æ•°æ®æº:
    - é€’é€ç³»ç»Ÿæå–_export_2026-01-27 (1).csv â†’ biomaterials collection (category: delivery_system)
    - å¾®ç”Ÿç‰©æå–_export_2026-01-29 (1).csv â†’ biomaterials collection (category: microbe)
    â†’ documents collection (å»é‡åˆå¹¶çš„è®ºæ–‡ä¿¡æ¯)
"""

import csv
import json
import asyncio
import sys
import argparse
from datetime import datetime
from typing import Dict, List, Any
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from motor.motor_asyncio import AsyncIOMotorClient
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv(Path(__file__).parent.parent / ".env")

# =============================================
# é…ç½®
# =============================================
MONGODB_URL = os.getenv("MONGODB_URL", "mongodb://localhost:27017")
DATABASE_NAME = os.getenv("MONGODB_DB_NAME", "biomedical_platform")

# CSV æ–‡ä»¶è·¯å¾„ (ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•)
PROJECT_ROOT = Path(__file__).parent.parent.parent
DELIVERY_CSV = PROJECT_ROOT / "é€’é€ç³»ç»Ÿæå–_export_2026-01-27 (1).csv"
MICROBE_CSV = PROJECT_ROOT / "å¾®ç”Ÿç‰©æå–_export_2026-01-29 (1).csv"


class DatabaseInitializer:
    """æ•°æ®åº“åˆå§‹åŒ–å™¨"""
    
    def __init__(self, mongo_uri: str, db_name: str, dry_run: bool = False):
        self.mongo_uri = mongo_uri
        self.db_name = db_name
        self.dry_run = dry_run
        self.client = None
        self.db = None
        
        # ç»Ÿè®¡
        self.stats = {
            "documents_total": 0,
            "biomaterials_delivery": 0,
            "biomaterials_microbe": 0,
            "errors": 0,
        }
    
    async def connect(self):
        """è¿æ¥ MongoDB"""
        self.client = AsyncIOMotorClient(self.mongo_uri)
        self.db = self.client[self.db_name]
        print(f"âœ“ Connected to MongoDB: {self.mongo_uri}")
        print(f"  Database: {self.db_name}")
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.client:
            self.client.close()
    
    def parse_csv(self, csv_path: Path) -> List[Dict[str, Any]]:
        """è§£æ CSV æ–‡ä»¶"""
        records = []
        
        # å¢åŠ å­—æ®µå¤§å°é™åˆ¶ï¼Œé˜²æ­¢å¤§ JSON å­—æ®µæŠ¥é”™
        csv.field_size_limit(sys.maxsize)
        
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # è§£æ features JSON (ç¬¬6åˆ—)
                features_str = row.get('features', '{}')
                try:
                    features = json.loads(features_str)
                except json.JSONDecodeError:
                    features = {}
                
                records.append({
                    "paper_id": row.get("paper_id", ""),
                    "title": row.get("title", ""),
                    "authors": row.get("authors", ""),
                    "journal": row.get("journal", ""),
                    "publish_year": int(row.get("publish_year", 0)) if row.get("publish_year", "").isdigit() else 0,
                    "features": features,
                })
        return records
    
    async def create_indexes(self):
        """åˆ›å»ºæ•°æ®åº“ç´¢å¼•ï¼ˆä¸ºæ‰€æœ‰é›†åˆåˆ›å»ºå¿…è¦ç´¢å¼•ï¼‰"""
        print("\nğŸ“Š Creating indexes for all collections...")
        
        if self.dry_run:
            print("   Skipped (dry run)")
            return
        
        # =============================================
        # æ ¸å¿ƒä¸šåŠ¡æ•°æ®è¡¨
        # =============================================
        
        # documents collection - è®ºæ–‡æ–‡çŒ®
        docs = self.db["documents"]
        await docs.create_index("paper_id", unique=True)
        await docs.create_index("source_tables")
        await docs.create_index([("title", "text"), ("authors", "text")])
        print("   âœ“ documents")
        
        # biomaterials collection - ç”Ÿç‰©ææ–™
        bio = self.db["biomaterials"]
        await bio.create_index("name", unique=True)
        await bio.create_index("category")
        await bio.create_index("subcategory")
        await bio.create_index("paper_ids")
        await bio.create_index([("name", "text"), ("paper_titles", "text")])
        print("   âœ“ biomaterials")
        
        # paper_tags collection - è®ºæ–‡æ ‡ç­¾
        tags = self.db["paper_tags"]
        await tags.create_index("paper_id", unique=True)
        await tags.create_index("l1")
        await tags.create_index("l2")
        await tags.create_index("classification")
        print("   âœ“ paper_tags")
        
        # atps_records collection - ATPS è®°å½•
        atps = self.db["atps_records"]
        await atps.create_index("polymer1")
        await atps.create_index("polymer2")
        await atps.create_index([("polymer1", 1), ("polymer2", 1)])
        print("   âœ“ atps_records")
        
        # assemblies collection - ç»„è£…ä½“
        assemblies = self.db["assemblies"]
        await assemblies.create_index("system_id", unique=True)
        await assemblies.create_index("category")
        await assemblies.create_index("paper_id")
        print("   âœ“ assemblies")
        
        # =============================================
        # ç”¨æˆ·ä¸è®¤è¯
        # =============================================
        
        # users collection - ç”¨æˆ·
        users = self.db["users"]
        await users.create_index("username", unique=True)
        await users.create_index("email", unique=True)
        await users.create_index("role")
        await users.create_index("is_active")
        print("   âœ“ users")
        
        # =============================================
        # å¯¹è¯ç®¡ç†
        # =============================================
        
        # conversations collection - å¯¹è¯
        convos = self.db["conversations"]
        await convos.create_index("id", unique=True)
        await convos.create_index("user_id")
        await convos.create_index("created_at")
        await convos.create_index("updated_at")
        await convos.create_index([("user_id", 1), ("created_at", -1)])
        await convos.create_index([("user_id", 1), ("updated_at", -1)])
        print("   âœ“ conversations")
        
        # messages collection - æ¶ˆæ¯
        msgs = self.db["messages"]
        await msgs.create_index("id", unique=True)
        await msgs.create_index("conversation_id")
        await msgs.create_index("timestamp")
        await msgs.create_index([("conversation_id", 1), ("timestamp", 1)])
        print("   âœ“ messages")
        
        # =============================================
        # Agent ä¸ LLM é…ç½®
        # =============================================
        
        # agents collection - Agent é…ç½®
        agents = self.db["agents"]
        await agents.create_index("id", unique=True)
        await agents.create_index("name")
        await agents.create_index("is_active")
        await agents.create_index("created_at")
        print("   âœ“ agents")
        
        # llm_providers collection - LLM æä¾›å•†
        providers = self.db["llm_providers"]
        await providers.create_index("id", unique=True)
        await providers.create_index("name")
        await providers.create_index("provider_type")
        await providers.create_index("is_active")
        print("   âœ“ llm_providers")
        
        # prompts collection - æç¤ºè¯æ¨¡æ¿
        prompts = self.db["prompts"]
        await prompts.create_index("id", unique=True)
        await prompts.create_index("name")
        await prompts.create_index("category")
        await prompts.create_index("is_active")
        print("   âœ“ prompts")
        
        # =============================================
        # MCP é…ç½®
        # =============================================
        
        # mcp_configs collection - MCP å…¨å±€é…ç½®
        mcp_configs = self.db["mcp_configs"]
        await mcp_configs.create_index("id", unique=True)
        await mcp_configs.create_index("name")
        print("   âœ“ mcp_configs")
        
        # mcp_servers collection - MCP æœåŠ¡å™¨
        mcp_servers = self.db["mcp_servers"]
        await mcp_servers.create_index("id", unique=True)
        await mcp_servers.create_index("name")
        await mcp_servers.create_index("is_active")
        print("   âœ“ mcp_servers")
        
        # mcp_tools collection - MCP å·¥å…·
        mcp_tools = self.db["mcp_tools"]
        await mcp_tools.create_index("id", unique=True)
        await mcp_tools.create_index("server_id")
        await mcp_tools.create_index("name")
        await mcp_tools.create_index("is_enabled")
        print("   âœ“ mcp_tools")
        
        # =============================================
        # çŸ¥è¯†åº“ä¸æŠ€èƒ½
        # =============================================
        
        # knowledge_bases collection - çŸ¥è¯†åº“
        kb = self.db["knowledge_bases"]
        await kb.create_index("id", unique=True)
        await kb.create_index("name")
        await kb.create_index("category")
        await kb.create_index("created_at")
        print("   âœ“ knowledge_bases")
        
        # skills collection - æŠ€èƒ½
        skills = self.db["skills"]
        await skills.create_index("id", unique=True)
        await skills.create_index("name")
        await skills.create_index("category")
        await skills.create_index("is_active")
        print("   âœ“ skills")
        
        # =============================================
        # æ–‡ä»¶ä¸ä»»åŠ¡
        # =============================================
        
        # files collection - æ–‡ä»¶å­˜å‚¨
        files = self.db["files"]
        await files.create_index("id", unique=True)
        await files.create_index("filename")
        await files.create_index("user_id")
        await files.create_index("created_at")
        print("   âœ“ files")
        
        # ocr_tasks collection - OCR ä»»åŠ¡
        ocr = self.db["ocr_tasks"]
        await ocr.create_index("id", unique=True)
        await ocr.create_index("file_id")
        await ocr.create_index("status")
        await ocr.create_index("created_at")
        print("   âœ“ ocr_tasks")
        
        # playground_sessions collection - Playground ä¼šè¯
        playground = self.db["playground_sessions"]
        await playground.create_index("id", unique=True)
        await playground.create_index("user_id")
        await playground.create_index("created_at")
        print("   âœ“ playground_sessions")
        
        print("\n   âœ“ All 18 collections initialized with indexes")
    
    async def import_documents(self, delivery_records: List[Dict], microbe_records: List[Dict]):
        """å¯¼å…¥æ–‡çŒ®è¡¨ (å»é‡åˆå¹¶)"""
        print("\nğŸ“š Importing documents...")
        
        # åˆå¹¶å¹¶å»é‡
        papers: Dict[str, Dict] = {}
        
        for record in delivery_records:
            pid = record["paper_id"]
            if not pid:
                continue
            if pid not in papers:
                papers[pid] = {
                    "paper_id": pid,
                    "title": record["title"],
                    "authors": record["authors"],
                    "journal": record["journal"],
                    "publish_year": record["publish_year"],
                    "source_tables": ["delivery"],
                    "created_at": datetime.now(),
                }
            else:
                if "delivery" not in papers[pid]["source_tables"]:
                    papers[pid]["source_tables"].append("delivery")
        
        for record in microbe_records:
            pid = record["paper_id"]
            if not pid:
                continue
            if pid not in papers:
                papers[pid] = {
                    "paper_id": pid,
                    "title": record["title"],
                    "authors": record["authors"],
                    "journal": record["journal"],
                    "publish_year": record["publish_year"],
                    "source_tables": ["microbe"],
                    "created_at": datetime.now(),
                }
            else:
                if "microbe" not in papers[pid]["source_tables"]:
                    papers[pid]["source_tables"].append("microbe")
        
        print(f"   Unique papers: {len(papers)}")
        
        if not self.dry_run and papers:
            docs_collection = self.db["documents"]
            await docs_collection.delete_many({})
            
            # æ‰¹é‡æ’å…¥
            batch_size = 1000
            paper_list = list(papers.values())
            for i in range(0, len(paper_list), batch_size):
                batch = paper_list[i:i + batch_size]
                await docs_collection.insert_many(batch)
                print(f"   Inserted batch: {i + len(batch)}/{len(paper_list)}")
        
        self.stats["documents_total"] = len(papers)
        print(f"   âœ“ Imported {len(papers)} documents")
        return papers
    
    @staticmethod
    def _merge_functional(dst: Dict, src: Dict) -> None:
        """åˆå¹¶ functional_performance / biological_impact å­—æ®µ"""
        if not src:
            return
        for k, v in src.items():
            if v is None or v == "":
                continue
            if isinstance(v, dict):
                if k not in dst:
                    dst[k] = dict(v)
                continue
            if k not in dst:
                dst[k] = v
            elif isinstance(v, str) and isinstance(dst[k], str):
                dst[k] = dst[k] + "; " + v

    async def import_biomaterials(self, delivery_records: List[Dict], microbe_records: List[Dict]):
        """å¯¼å…¥ç”Ÿç‰©ææ–™è¡¨ï¼ˆæŒ‰ææ–™åç§°èšåˆï¼Œä¸€ä¸ªææ–™å¯¹åº”å¤šç¯‡è®ºæ–‡ï¼‰"""
        print("\nğŸ§¬ Importing biomaterials (aggregated by name)...")
        
        # æŒ‰ææ–™åç§°èšåˆ
        material_map: Dict[str, Dict[str, Any]] = {}
        
        def ensure_material(name: str, category: str, subcategory: str) -> Dict:
            if name not in material_map:
                material_map[name] = {
                    "name": name,
                    "category": category,
                    "subcategory": subcategory,
                    "paper_ids": set(),
                    "functional_performance": {},
                    "biological_impact": {},
                    "raw_data": {},
                }
            return material_map[name]
        
        # å¤„ç†é€’é€ç³»ç»Ÿ
        print("   Processing delivery systems...")
        for record in delivery_records:
            features = record.get("features", {})
            paper_id = record["paper_id"]
            
            # ä» assemblies ä¸­æå–ææ–™
            for asm in features.get("assemblies", []):
                composition = asm.get("composition", {})
                mat_name = (composition.get("material_name") or "").strip()
                if not mat_name:
                    continue
                
                sub = asm.get("system_category", "unknown")
                ent = ensure_material(mat_name, "delivery_system", sub)
                ent["paper_ids"].add(paper_id)
                
                # åˆå¹¶ functional_performance
                fp = asm.get("functional_performance", {})
                if fp:
                    self._merge_functional(ent["functional_performance"], fp)
                
                # åˆå¹¶ biological_impact
                bio = asm.get("biological_impact_on_host", {})
                if bio:
                    self._merge_functional(ent["biological_impact"], bio)
                
                # ä¿ç•™ç¬¬ä¸€æ¡çš„ raw_data
                if not ent["raw_data"]:
                    ent["raw_data"] = asm
            
            # ä» materials åˆ—è¡¨æå–ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            for m in features.get("materials", []):
                name = (m.get("standardized_name") or "").strip()
                if not name:
                    continue
                identity = m.get("identity", {})
                sub = (identity.get("material_type") or "unknown").strip()
                ent = ensure_material(name, "delivery_system", sub)
                ent["paper_ids"].add(paper_id)
        
        delivery_count = sum(1 for e in material_map.values() if e["category"] == "delivery_system")
        self.stats["biomaterials_delivery"] = delivery_count
        print(f"   Delivery systems: {delivery_count} (aggregated)")
        
        # å¤„ç†å¾®ç”Ÿç‰©
        print("   Processing microbes...")
        for record in microbe_records:
            features = record.get("features", {})
            paper_id = record["paper_id"]
            
            for mic in features.get("microbes", []):
                std_name = (mic.get("standardized_name") or "").strip()
                if not std_name:
                    continue
                
                identity = mic.get("identity", {})
                sub = (identity.get("type") or "unknown").strip()
                ent = ensure_material(std_name, "microbe", sub)
                ent["paper_ids"].add(paper_id)
                
                # ä¿ç•™å®Œæ•´å¾®ç”Ÿç‰©æ•°æ®ä½œä¸º raw_data
                if not ent["raw_data"]:
                    ent["raw_data"] = dict(mic)
                
                # æå– functionality notes
                for tm in (mic.get("effector_modules") or {}).get("therapeutic_mechanisms") or []:
                    notes = (tm or {}).get("mechanism_notes")
                    if notes:
                        self._merge_functional(ent["functional_performance"], {"functionality_notes": notes})
        
        microbe_count = sum(1 for e in material_map.values() if e["category"] == "microbe")
        self.stats["biomaterials_microbe"] = microbe_count
        print(f"   Microbes: {microbe_count} (aggregated)")
        
        # æ„å»ºæ–‡çŒ® ID â†’ æ ‡é¢˜ æ˜ å°„
        paper_title_map = {}
        for record in delivery_records + microbe_records:
            pid = record.get("paper_id", "")
            if pid and pid not in paper_title_map:
                paper_title_map[pid] = record.get("title", "")
        
        # è½¬æ¢ä¸ºå†™å…¥æ ¼å¼
        biomaterials = []
        for name, ent in material_map.items():
            paper_list = list(ent["paper_ids"])
            paper_titles = [paper_title_map.get(pid, "") for pid in paper_list]
            
            biomaterials.append({
                "name": name,
                "category": ent["category"],
                "subcategory": ent["subcategory"],
                "paper_ids": paper_list,
                "paper_count": len(paper_list),
                "paper_titles": paper_titles,
                "functional_performance": ent["functional_performance"],
                "biological_impact": ent["biological_impact"],
                "raw_data": ent["raw_data"],
                "created_at": datetime.now(),
            })
        
        # å†™å…¥æ•°æ®åº“
        if not self.dry_run and biomaterials:
            bio_collection = self.db["biomaterials"]
            await bio_collection.delete_many({})
            
            batch_size = 1000
            for i in range(0, len(biomaterials), batch_size):
                batch = biomaterials[i:i + batch_size]
                await bio_collection.insert_many(batch)
                print(f"   Inserted batch: {i + len(batch)}/{len(biomaterials)}")
        
        total = len(material_map)
        multi_paper = sum(1 for b in biomaterials if b["paper_count"] > 1)
        print(f"   âœ“ Imported {total} biomaterials (aggregated)")
        print(f"     - {multi_paper} materials linked to multiple papers")
        return biomaterials
    
    async def create_default_admin(self):
        """åˆ›å»ºé»˜è®¤ç®¡ç†å‘˜ç”¨æˆ·"""
        print("\nğŸ‘¤ Creating default admin user...")
        
        if self.dry_run:
            print("   Skipped (dry run)")
            return
        
        from passlib.context import CryptContext
        pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
        
        users = self.db["users"]
        existing = await users.find_one({"username": "admin"})
        
        if existing:
            print("   Admin user already exists, skipping")
            return
        
        admin_user = {
            "id": "admin-001",
            "username": "admin",
            "email": "admin@example.com",
            "hashed_password": pwd_context.hash("admin123"),
            "full_name": "ç³»ç»Ÿç®¡ç†å‘˜",
            "role": "admin",
            "is_active": True,
            "created_at": datetime.now(),
        }
        
        await users.insert_one(admin_user)
        print("   âœ“ Default admin created (username: admin, password: admin123)")
        print("   âš ï¸  è¯·åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç«‹å³ä¿®æ”¹é»˜è®¤å¯†ç !")
    
    async def run(self):
        """æ‰§è¡Œå®Œæ•´åˆå§‹åŒ–æµç¨‹"""
        print("=" * 60)
        print("Bio-Agent Database Initialization")
        print("=" * 60)
        print(f"MongoDB: {self.mongo_uri}")
        print(f"Database: {self.db_name}")
        print(f"Dry Run: {self.dry_run}")
        print()
        
        # æ£€æŸ¥ CSV æ–‡ä»¶
        print("ğŸ“ Checking data files...")
        if not DELIVERY_CSV.exists():
            print(f"   âš ï¸  Delivery CSV not found: {DELIVERY_CSV}")
            print("   Skipping delivery data import")
            delivery_records = []
        else:
            print(f"   âœ“ Delivery CSV: {DELIVERY_CSV.stat().st_size / (1024*1024):.1f} MB")
            delivery_records = self.parse_csv(DELIVERY_CSV)
            print(f"     Parsed: {len(delivery_records)} rows")
        
        if not MICROBE_CSV.exists():
            print(f"   âš ï¸  Microbe CSV not found: {MICROBE_CSV}")
            print("   Skipping microbe data import")
            microbe_records = []
        else:
            print(f"   âœ“ Microbe CSV: {MICROBE_CSV.stat().st_size / (1024*1024):.1f} MB")
            microbe_records = self.parse_csv(MICROBE_CSV)
            print(f"     Parsed: {len(microbe_records)} rows")
        
        await self.connect()
        
        try:
            # å¯¼å…¥æ•°æ®
            if delivery_records or microbe_records:
                await self.import_documents(delivery_records, microbe_records)
                await self.import_biomaterials(delivery_records, microbe_records)
            
            # åˆ›å»ºç´¢å¼•
            await self.create_indexes()
            
            # åˆ›å»ºé»˜è®¤ç®¡ç†å‘˜
            await self.create_default_admin()
            
            # æ‰“å°ç»Ÿè®¡
            print("\n" + "=" * 60)
            print("ğŸ“Š Summary")
            print("=" * 60)
            print(f"Documents:        {self.stats['documents_total']}")
            print(f"Biomaterials:     {self.stats['biomaterials_delivery'] + self.stats['biomaterials_microbe']}")
            print(f"  - Delivery:     {self.stats['biomaterials_delivery']}")
            print(f"  - Microbes:     {self.stats['biomaterials_microbe']}")
            print()
            
            if self.dry_run:
                print("âš ï¸  This was a dry run. No data was actually written.")
            else:
                print("âœ… Database initialization completed successfully!")
            
        finally:
            await self.close()


async def main():
    parser = argparse.ArgumentParser(description="Initialize Bio-Agent database")
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Parse files and show statistics without importing"
    )
    args = parser.parse_args()
    
    initializer = DatabaseInitializer(MONGODB_URL, DATABASE_NAME, dry_run=args.dry_run)
    await initializer.run()


if __name__ == "__main__":
    asyncio.run(main())
