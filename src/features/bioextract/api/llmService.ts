/**
 * BioExtract-AI LLM æœåŠ¡
 * è°ƒç”¨çœŸå®å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ™ºèƒ½ç­›é€‰å’Œæ¨è
 */

// LLM é…ç½®æ¥å£
export interface LLMConfig {
    provider: 'openai' | 'gemini' | 'anthropic' | 'deepseek' | 'local';
    apiKey: string;
    baseUrl?: string;
    model: string;
    temperature?: number;
    maxTokens?: number;
}

// æ¶ˆæ¯æ ¼å¼
export interface ChatMessage {
    role: 'system' | 'user' | 'assistant';
    content: string;
}

// LLM å“åº”
export interface LLMResponse {
    content: string;
    usage?: {
        promptTokens: number;
        completionTokens: number;
        totalTokens: number;
    };
}

// =============================================
// é…ç½®å¸¸é‡
// =============================================
const LLM_DEFAULT_TEMPERATURE = 0.7;
const LLM_DEFAULT_MAX_TOKENS = 16384;
const LLM_CONFIG_STORAGE_KEY = 'bioextract_llm_config';

// é»˜è®¤é…ç½®
const DEFAULT_CONFIG: Partial<LLMConfig> = {
    temperature: LLM_DEFAULT_TEMPERATURE,
    maxTokens: LLM_DEFAULT_MAX_TOKENS,
};

// =============================================
// Provider ç¼“å­˜ï¼ˆä»åç«¯ API åŒæ­¥ï¼‰
// =============================================
const PROVIDERS_CACHE_KEY = 'llm_providers_cache';
const PROVIDERS_CACHE_TTL = 60 * 1000; // 1 åˆ†é’Ÿç¼“å­˜

interface ProviderCache {
    providers: Array<{
        id: string;
        name: string;
        baseUrl: string;
        apiKey: string;
        models: string[];
        isEnabled: boolean;
    }>;
    updatedAt: number;
}

// è·å–ç¼“å­˜çš„ providers
function getCachedProviders(): ProviderCache | null {
    try {
        const cached = localStorage.getItem(PROVIDERS_CACHE_KEY);
        if (cached) {
            const data = JSON.parse(cached) as ProviderCache;
            // æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            if (Date.now() - data.updatedAt < PROVIDERS_CACHE_TTL) {
                return data;
            }
        }
    } catch (e) {
        console.error('Failed to load providers cache:', e);
    }
    return null;
}

// åŒæ­¥ providers åˆ°ç¼“å­˜ï¼ˆä½¿ç”¨å†…éƒ¨ç«¯ç‚¹è·å–å®Œæ•´ API Keyï¼‰
export async function syncProviders(): Promise<void> {
    try {
        // ä½¿ç”¨ internal ç«¯ç‚¹è·å–æœªæ©ç çš„ API Key
        const res = await fetch('/api/v1/config/providers/internal');
        if (res.ok) {
            const providers = await res.json();
            const cache: ProviderCache = {
                providers,
                updatedAt: Date.now(),
            };
            localStorage.setItem(PROVIDERS_CACHE_KEY, JSON.stringify(cache));
        }
    } catch (e) {
        console.error('Failed to sync providers:', e);
    }
}

// =============================================
// System Settings ç¼“å­˜ï¼ˆé»˜è®¤æ¨¡å‹é…ç½®ï¼‰
// =============================================
const SYSTEM_SETTINGS_CACHE_KEY = 'llm_system_settings_cache';
const SYSTEM_SETTINGS_CACHE_TTL = 60 * 1000; // 1 åˆ†é’Ÿ

interface SystemSettingsCache {
    defaultProviderId: string | null;
    defaultModel: string | null;
    updatedAt: number;
}

function getCachedSystemSettings(): SystemSettingsCache | null {
    try {
        const cached = localStorage.getItem(SYSTEM_SETTINGS_CACHE_KEY);
        if (cached) {
            const data = JSON.parse(cached) as SystemSettingsCache;
            if (Date.now() - data.updatedAt < SYSTEM_SETTINGS_CACHE_TTL) {
                return data;
            }
        }
    } catch { /* ignore */ }
    return null;
}

// åŒæ­¥ç³»ç»Ÿè®¾ç½®åˆ°ç¼“å­˜
export async function syncSystemSettings(): Promise<void> {
    try {
        const res = await fetch('/api/v1/config/settings');
        if (res.ok) {
            const data = await res.json();
            const cache: SystemSettingsCache = {
                defaultProviderId: data.defaultProviderId || null,
                defaultModel: data.defaultModel || null,
                updatedAt: Date.now(),
            };
            localStorage.setItem(SYSTEM_SETTINGS_CACHE_KEY, JSON.stringify(cache));
        }
    } catch (e) {
        console.error('Failed to sync system settings:', e);
    }
}

// ä» localStorage æˆ–åç«¯è·å–é…ç½®
export function getLLMConfig(): LLMConfig | null {
    const cache = getCachedProviders();
    const systemSettings = getCachedSystemSettings();

    // 1. ä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½®çš„é»˜è®¤æ¨¡å‹
    if (systemSettings?.defaultProviderId && systemSettings?.defaultModel && cache) {
        const defaultProvider = cache.providers.find(
            p => p.id === systemSettings.defaultProviderId && p.isEnabled
        );
        if (defaultProvider) {
            const providerType = detectProviderType(defaultProvider.name, defaultProvider.baseUrl);
            return {
                provider: providerType,
                apiKey: defaultProvider.apiKey,
                baseUrl: defaultProvider.baseUrl,
                model: systemSettings.defaultModel,
                temperature: DEFAULT_CONFIG.temperature,
                maxTokens: DEFAULT_CONFIG.maxTokens,
            };
        }
    }

    // 2. å›é€€ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„ provider çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
    if (cache && cache.providers.length > 0) {
        const enabledProvider = cache.providers.find(p => p.isEnabled);
        if (enabledProvider) {
            const providerType = detectProviderType(enabledProvider.name, enabledProvider.baseUrl);
            return {
                provider: providerType,
                apiKey: enabledProvider.apiKey,
                baseUrl: enabledProvider.baseUrl,
                model: enabledProvider.models?.[0] || 'gpt-4o-mini',
                temperature: DEFAULT_CONFIG.temperature,
                maxTokens: DEFAULT_CONFIG.maxTokens,
            };
        }
    }

    // 3. æœ€åå›é€€ï¼šæ—§çš„ localStorage é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
    try {
        const stored = localStorage.getItem(LLM_CONFIG_STORAGE_KEY);
        if (stored) {
            const config = JSON.parse(stored);
            if (config.apiKey) {
                return config;
            }
        }
    } catch (e) {
        console.error('Failed to load LLM config:', e);
    }

    return null;
}

// æ ¹æ® provider name æˆ– baseUrl æ¨æ–­ provider type
function detectProviderType(name: string, baseUrl: string): LLMConfig['provider'] {
    const lowerName = name.toLowerCase();
    const lowerUrl = baseUrl.toLowerCase();

    if (lowerName.includes('gemini') || lowerUrl.includes('googleapis')) {
        return 'gemini';
    }
    if (lowerName.includes('claude') || lowerName.includes('anthropic') || lowerUrl.includes('anthropic')) {
        return 'anthropic';
    }
    if (lowerName.includes('deepseek') || lowerUrl.includes('deepseek')) {
        return 'deepseek';
    }
    if (lowerUrl.includes('localhost') || lowerUrl.includes('127.0.0.1')) {
        return 'local';
    }
    // é»˜è®¤ä½¿ç”¨ OpenAI å…¼å®¹
    return 'openai';
}

// ä¿å­˜é…ç½®åˆ° localStorage
export function saveLLMConfig(config: LLMConfig): void {
    localStorage.setItem(LLM_CONFIG_STORAGE_KEY, JSON.stringify(config));
}

// æ„å»ºç³»ç»Ÿæç¤ºè¯
export function buildSystemPrompt(dataContext: string): string {
    return `ä½ æ˜¯ BioExtract-AIï¼Œä¸€ä¸ªä¸“ä¸šçš„ç”Ÿç‰©ææ–™æ™ºèƒ½ç­›é€‰åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºç”¨æˆ·æä¾›çš„éœ€æ±‚ï¼Œä»å·²åŠ è½½çš„è¯ç‰©é€’é€æ•°æ®åº“ä¸­ç­›é€‰åˆé€‚çš„èšåˆç‰©ææ–™å’Œè½½ä½“è®¾è®¡æ–¹æ¡ˆï¼Œå¹¶ç»™å‡ºä¸“ä¸šçš„æ¨èã€‚

## ä½ çš„èƒ½åŠ›ï¼š
1. **Drug Delivery æ•°æ®åº“æŸ¥è¯¢**ï¼šä½ å¯ä»¥æŸ¥è¯¢ drug_delivery.csv ä¸­çš„è¯ç‰©é€’é€æ–‡çŒ®æ•°æ®ï¼ŒåŒ…å«èšåˆç‰©åç§°ã€è½½ä½“å½¢æ€ã€å“åº”æœºåˆ¶ã€è´Ÿè½½ç‰©ä¿¡æ¯ã€é‡Šæ”¾ç‰¹æ€§ç­‰
2. **èšåˆç‰©ç­›é€‰**ï¼šæ ¹æ®ç”¨æˆ·éœ€æ±‚ç­›é€‰åˆé€‚çš„èšåˆç‰©ææ–™ï¼ˆå¦‚ PEGã€å£³èšç³–ã€æµ·è—»é…¸ç›ç­‰ï¼‰
3. **è½½ä½“è®¾è®¡æ¨è**ï¼šæ¨èé€‚åˆçš„è½½ä½“å½¢æ€ï¼ˆæ°´å‡èƒ¶ã€å¾®çƒã€çº³ç±³é¢—ç²’ç­‰ï¼‰
4. **å“åº”æœºåˆ¶åˆ†æ**ï¼šåˆ†æ pH å“åº”ã€é…¶å“åº”ã€æ¸©åº¦å“åº”ç­‰æ™ºèƒ½é‡Šæ”¾æœºåˆ¶
5. **æ–‡çŒ®æº¯æº**ï¼šåŸºäºæ•°æ®åº“ä¸­çš„è®ºæ–‡ä¿¡æ¯æä¾›æ–‡çŒ®å‚è€ƒ

## å›ç­”æ ¼å¼è¦æ±‚ï¼š
1. ä½¿ç”¨ Markdown æ ¼å¼ï¼ŒåŒ…å«æ¸…æ™°çš„æ ‡é¢˜å’Œåˆ†ç‚¹
2. å¯¹äºæ¨èæ–¹æ¡ˆï¼Œå¿…é¡»è¯´æ˜æ¯ä¸ªç»„åˆ†çš„ä½œç”¨æœºåˆ¶
3. ä½¿ç”¨è¡¨æƒ…ç¬¦å·å¢å¼ºå¯è¯»æ€§ï¼ˆå¦‚ âœ… âŒ âš ï¸ ğŸ“Š ğŸ§¬ ğŸ’Šï¼‰
4. å¼•ç”¨æ•°æ®åº“ä¸­çš„å…·ä½“æ¡ˆä¾‹æ—¶ï¼Œè¯´æ˜æ–‡çŒ®æ¥æº
5. å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ç›´æ¥åŒ¹é…çš„è®°å½•ï¼ŒåŸºäºä¸“ä¸šçŸ¥è¯†ç»™å‡ºæ¨èå¹¶æ³¨æ˜

## æ•°æ®åº“å­—æ®µè¯´æ˜ï¼š
- **è½½ä½“è®¾è®¡_èšåˆç‰©åç§°**: ä½¿ç”¨çš„èšåˆç‰©ææ–™
- **è½½ä½“è®¾è®¡_è½½ä½“å½¢æ€**: è½½ä½“ç»“æ„ï¼ˆæ°´å‡èƒ¶ã€å¾®çƒã€çº³ç±³é¢—ç²’ç­‰ï¼‰
- **è½½ä½“è®¾è®¡_å“åº”æœºåˆ¶**: æ™ºèƒ½å“åº”é‡Šæ”¾æœºåˆ¶
- **è´Ÿè½½ç‰©ä¿¡æ¯_åç§°/ç±»å‹/å½¢æ€çŠ¶æ€**: è´Ÿè½½è¯ç‰©æˆ–å¾®ç”Ÿç‰©ä¿¡æ¯
- **å¾®ç”Ÿç‰©æŒ‡æ ‡_åŒ…åŸ‹æ•ˆç‡/ä¿æŠ¤æ€§èƒ½/é‡Šæ”¾åæ´»æ€§/æ³„éœ²æ§åˆ¶**: å¾®ç”Ÿç‰©é€’é€ç›¸å…³æŒ‡æ ‡
- **é‡Šæ”¾ç‰¹æ€§_è§¦å‘æ¡ä»¶/é‡Šæ”¾åŠ¨åŠ›å­¦**: é‡Šæ”¾è¡Œä¸ºç‰¹å¾

## å½“å‰å·²åŠ è½½çš„æ•°æ®ä¸Šä¸‹æ–‡ï¼š
${dataContext}

è¯·åŸºäºä»¥ä¸Šæ•°æ®å’Œä½ çš„ä¸“ä¸šçŸ¥è¯†ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ä¸­çš„çœŸå®æ¡ˆä¾‹ä½œä¸ºæ¨èä¾æ®ã€‚`
}

// è°ƒç”¨ OpenAI å…¼å®¹ API
async function callOpenAICompatible(
    config: LLMConfig,
    messages: ChatMessage[]
): Promise<LLMResponse> {
    const baseUrl = config.baseUrl || 'https://api.openai.com/v1';

    const response = await fetch(`${baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${config.apiKey}`,
        },
        body: JSON.stringify({
            model: config.model,
            messages: messages.map(m => ({ role: m.role, content: m.content })),
            temperature: config.temperature ?? DEFAULT_CONFIG.temperature,
            max_tokens: config.maxTokens ?? DEFAULT_CONFIG.maxTokens,
        }),
    });

    if (!response.ok) {
        const error = await response.text();
        throw new Error(`LLM API error: ${response.status} - ${error}`);
    }

    const data = await response.json();
    return {
        content: data.choices[0]?.message?.content || '',
        usage: data.usage ? {
            promptTokens: data.usage.prompt_tokens,
            completionTokens: data.usage.completion_tokens,
            totalTokens: data.usage.total_tokens,
        } : undefined,
    };
}

// è°ƒç”¨ Gemini API
async function callGemini(
    config: LLMConfig,
    messages: ChatMessage[]
): Promise<LLMResponse> {
    const baseUrl = config.baseUrl || 'https://generativelanguage.googleapis.com/v1beta';

    // è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸º Gemini æ ¼å¼
    const contents = messages
        .filter(m => m.role !== 'system')
        .map(m => ({
            role: m.role === 'assistant' ? 'model' : 'user',
            parts: [{ text: m.content }],
        }));

    // ç³»ç»ŸæŒ‡ä»¤
    const systemInstruction = messages.find(m => m.role === 'system')?.content;

    const response = await fetch(
        `${baseUrl}/models/${config.model}:generateContent?key=${config.apiKey}`,
        {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({
                contents,
                systemInstruction: systemInstruction ? { parts: [{ text: systemInstruction }] } : undefined,
                generationConfig: {
                    temperature: config.temperature ?? DEFAULT_CONFIG.temperature,
                    maxOutputTokens: config.maxTokens ?? DEFAULT_CONFIG.maxTokens,
                },
            }),
        }
    );

    if (!response.ok) {
        const error = await response.text();
        throw new Error(`Gemini API error: ${response.status} - ${error}`);
    }

    const data = await response.json();
    const content = data.candidates?.[0]?.content?.parts?.[0]?.text || '';

    return {
        content,
        usage: data.usageMetadata ? {
            promptTokens: data.usageMetadata.promptTokenCount || 0,
            completionTokens: data.usageMetadata.candidatesTokenCount || 0,
            totalTokens: data.usageMetadata.totalTokenCount || 0,
        } : undefined,
    };
}

// è°ƒç”¨ Anthropic Claude API
async function callAnthropic(
    config: LLMConfig,
    messages: ChatMessage[]
): Promise<LLMResponse> {
    const baseUrl = config.baseUrl || 'https://api.anthropic.com/v1';

    // æå–ç³»ç»Ÿæ¶ˆæ¯
    const systemMessage = messages.find(m => m.role === 'system')?.content || '';
    const chatMessages = messages.filter(m => m.role !== 'system');

    const response = await fetch(`${baseUrl}/messages`, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'x-api-key': config.apiKey,
            'anthropic-version': '2023-06-01',
        },
        body: JSON.stringify({
            model: config.model,
            max_tokens: config.maxTokens ?? DEFAULT_CONFIG.maxTokens,
            system: systemMessage,
            messages: chatMessages.map(m => ({
                role: m.role,
                content: m.content,
            })),
        }),
    });

    if (!response.ok) {
        const error = await response.text();
        throw new Error(`Anthropic API error: ${response.status} - ${error}`);
    }

    const data = await response.json();
    return {
        content: data.content?.[0]?.text || '',
        usage: data.usage ? {
            promptTokens: data.usage.input_tokens,
            completionTokens: data.usage.output_tokens,
            totalTokens: data.usage.input_tokens + data.usage.output_tokens,
        } : undefined,
    };
}

// ä¸»è°ƒç”¨å‡½æ•°
export async function callLLM(
    config: LLMConfig,
    messages: ChatMessage[]
): Promise<LLMResponse> {
    switch (config.provider) {
        case 'openai':
        case 'deepseek':
        case 'local':
            return callOpenAICompatible(config, messages);
        case 'gemini':
            return callGemini(config, messages);
        case 'anthropic':
            return callAnthropic(config, messages);
        default:
            throw new Error(`Unsupported LLM provider: ${config.provider}`);
    }
}

// é¢„è®¾çš„æ¨¡å‹é€‰é¡¹ï¼ˆä½œä¸ºåå¤‡ï¼‰
export const LLM_PROVIDERS = [
    {
        id: 'openai',
        name: 'OpenAI',
        fallbackModels: ['gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-3.5-turbo'],
        defaultBaseUrl: 'https://api.openai.com/v1',
        supportsModelList: true,
    },
    {
        id: 'gemini',
        name: 'Google Gemini',
        fallbackModels: ['gemini-2.0-flash-exp', 'gemini-1.5-pro', 'gemini-1.5-flash'],
        defaultBaseUrl: 'https://generativelanguage.googleapis.com/v1beta',
        supportsModelList: true,
    },
    {
        id: 'anthropic',
        name: 'Anthropic Claude',
        fallbackModels: ['claude-3-5-sonnet-20241022', 'claude-3-haiku-20240307', 'claude-3-opus-20240229'],
        defaultBaseUrl: 'https://api.anthropic.com/v1',
        supportsModelList: false, // Anthropic ä¸æä¾›å…¬å¼€çš„æ¨¡å‹åˆ—è¡¨ API
    },
    {
        id: 'deepseek',
        name: 'DeepSeek',
        fallbackModels: ['deepseek-chat', 'deepseek-coder', 'deepseek-reasoner'],
        defaultBaseUrl: 'https://api.deepseek.com/v1',
        supportsModelList: true,
    },
    {
        id: 'local',
        name: 'æœ¬åœ°/è‡ªå®šä¹‰',
        fallbackModels: [],
        defaultBaseUrl: 'http://localhost:11434/v1',
        supportsModelList: true,
    },
] as const;

// æ¨¡å‹ä¿¡æ¯æ¥å£
export interface ModelInfo {
    id: string;
    name: string;
    description?: string;
    contextLength?: number;
    created?: number;
}

// åŠ¨æ€è·å– OpenAI å…¼å®¹ API çš„æ¨¡å‹åˆ—è¡¨
async function fetchOpenAIModels(baseUrl: string, apiKey: string): Promise<ModelInfo[]> {
    try {
        const response = await fetch(`${baseUrl}/models`, {
            method: 'GET',
            headers: {
                'Authorization': `Bearer ${apiKey}`,
            },
        });

        if (!response.ok) {
            throw new Error(`Failed to fetch models: ${response.status}`);
        }

        const data = await response.json();
        const models: ModelInfo[] = (data.data || [])
            .filter((m: { id: string }) => {
                // è¿‡æ»¤æ‰éèŠå¤©æ¨¡å‹ï¼ˆå¦‚ embeddingã€whisperã€ttsã€image ç­‰ï¼‰
                const id = m.id.toLowerCase();

                // æ’é™¤å·²çŸ¥çš„éèŠå¤©æ¨¡å‹
                const excludePatterns = [
                    'embedding', 'embed', 'whisper', 'tts',
                    'dall-e', 'moderation', 'davinci', 'babbage',
                    'ada', 'curie', 'search', 'similarity',
                    'code-search', 'text-search', 'edit',
                ];
                if (excludePatterns.some(p => id.includes(p))) {
                    return false;
                }

                // åŒ…å«å·²çŸ¥çš„èŠå¤©æ¨¡å‹ç³»åˆ—
                const includePatterns = [
                    'gpt', 'chat', 'turbo',
                    'deepseek', 'qwen', 'llama', 'mistral', 'claude',
                    'gemini', 'gemma',
                    'o1', 'o3', 'o4',
                    'yi', 'glm', 'baichuan', 'moonshot',
                ];
                return includePatterns.some(p => id.includes(p));
            })
            .map((m: { id: string; created?: number; owned_by?: string }) => ({
                id: m.id,
                name: m.id,
                created: m.created,
                description: m.owned_by,
            }))
            .sort((a: ModelInfo, b: ModelInfo) => (b.created || 0) - (a.created || 0));

        return models;
    } catch (error) {
        console.error('Failed to fetch OpenAI models:', error);
        return [];
    }
}

// åŠ¨æ€è·å– Gemini æ¨¡å‹åˆ—è¡¨
async function fetchGeminiModels(baseUrl: string, apiKey: string): Promise<ModelInfo[]> {
    try {
        const response = await fetch(`${baseUrl}/models?key=${apiKey}`, {
            method: 'GET',
        });

        if (!response.ok) {
            throw new Error(`Failed to fetch models: ${response.status}`);
        }

        const data = await response.json();
        const models: ModelInfo[] = (data.models || [])
            .filter((m: { name: string; supportedGenerationMethods?: string[] }) => {
                // åªä¿ç•™æ”¯æŒ generateContent çš„æ¨¡å‹
                return m.supportedGenerationMethods?.includes('generateContent');
            })
            .map((m: { name: string; displayName?: string; description?: string; inputTokenLimit?: number }) => ({
                id: m.name.replace('models/', ''),
                name: m.displayName || m.name.replace('models/', ''),
                description: m.description,
                contextLength: m.inputTokenLimit,
            }));

        return models;
    } catch (error) {
        console.error('Failed to fetch Gemini models:', error);
        return [];
    }
}

// ä¸»å‡½æ•°ï¼šæ ¹æ®æä¾›å•†è·å–æ¨¡å‹åˆ—è¡¨
export async function fetchAvailableModels(
    provider: LLMConfig['provider'],
    apiKey: string,
    baseUrl?: string
): Promise<ModelInfo[]> {
    const providerConfig = LLM_PROVIDERS.find(p => p.id === provider);
    if (!providerConfig) {
        return [];
    }

    const url = baseUrl || providerConfig.defaultBaseUrl;

    try {
        switch (provider) {
            case 'openai':
            case 'deepseek':
            case 'local':
                return await fetchOpenAIModels(url, apiKey);

            case 'gemini':
                return await fetchGeminiModels(url, apiKey);

            case 'anthropic':
                // Anthropic ä¸æ”¯æŒæ¨¡å‹åˆ—è¡¨ APIï¼Œè¿”å›é¢„è®¾åˆ—è¡¨
                return providerConfig.fallbackModels.map(id => ({ id, name: id }));

            default:
                return [];
        }
    } catch (error) {
        console.error(`Failed to fetch models for ${provider}:`, error);
        // è¿”å›åå¤‡æ¨¡å‹åˆ—è¡¨
        return providerConfig.fallbackModels.map(id => ({ id, name: id }));
    }
}

// è·å–åå¤‡æ¨¡å‹åˆ—è¡¨
export function getFallbackModels(provider: LLMConfig['provider']): string[] {
    const providerConfig = LLM_PROVIDERS.find(p => p.id === provider);
    return providerConfig ? [...providerConfig.fallbackModels] : [];
}

export default {
    callLLM,
    getLLMConfig,
    saveLLMConfig,
    buildSystemPrompt,
    fetchAvailableModels,
    getFallbackModels,
    LLM_PROVIDERS,
};
